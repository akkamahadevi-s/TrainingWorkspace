package com.training.sparkworks;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import static org.apache.spark.sql.functions.*;

import java.util.ArrayList;
import java.util.List;

import com.training.commons.SparkConnection;

public class SparkSQLDemo {
	public static void main(String[] args) {
		Logger.getLogger("org").setLevel(Level.ERROR);
		//optional
		Logger.getLogger("akka").setLevel(Level.ERROR);
		
		JavaSparkContext sparkContext = SparkConnection.getContext();
		SparkSession spSession = SparkConnection.getSession();
		Dataset<Row> empDatafields = spSession.read().json("./data/customerData.json");
		empDatafields.show();
		empDatafields.printSchema();
		
		//data queries
		System.out.println("SELECT Demo");
		empDatafields.select(col("name"),col("salary")).show();
		
		//data queries on selection(condition)
		System.out.println("Select Demo with Condition");
		empDatafields.filter(col("gender").equalTo("male")).show();
		
		//aggregate
		System.out.println("Aggregate-group by gender");
		empDatafields.groupBy(col("gender")).count().show();
		
		//group by dept id ,average salary and max age
		Dataset<Row> summaryData = empDatafields.groupBy(col("deptid"))
				.agg(avg(empDatafields.col("salary")),max(col("age")));
		
		summaryData.show();
		
		//---------try with bean classes---
		Department dept1 = new Department(100,"Development");
		Department dept2 = new Department(200,"Testing");
		List<Department> deptList = new ArrayList<Department>();
		deptList.add(dept1);
		deptList.add(dept2);
		
		Dataset<Row> deptDataFields = spSession.createDataFrame(deptList,Department.class);
		
		System.out.println("Department Contents are");
		deptDataFields.show();
		
		System.out.println("Join Employee with Dept");
		Dataset<Row> empDeptJoin = empDatafields.join(deptDataFields,col("deptid").equalTo(col("departmentId")));
		empDeptJoin.show();
		
		System.out.println("-----------Join with Aggregation------");
		empDatafields.filter(col("age").gt(30))
		.join(deptDataFields,col("deptid")
		.equalTo(col("departmentId"))).groupBy(col("deptid"))
		.agg( 
				avg(empDatafields.col("salary")),
				max(empDatafields.col("age"))
				).show();
		
		//loading from csv
		Dataset<Row> autoData = spSession.read().option("header","true").csv("./data/auto-data.csv");
		autoData.show(5);
		
		//creating RDD with row objects
		Row row1 = RowFactory.create(1,"India","Bengaluru");
		Row row2 = RowFactory.create(2,"USA","Reston");
		Row row3 = RowFactory.create(3,"UK","Steevenscreek");
		
		List<Row> rList = new ArrayList<Row>();
		
		rList.add(row1);
		rList.add(row2);
		rList.add(row3);
		
		JavaRDD<Row> rowRDD = sparkContext.parallelize(rList);
		StructType schema = DataTypes.createStructType(new StructField[] {
				DataTypes.createStructField("id",DataTypes.IntegerType,false),
				DataTypes.createStructField("country",DataTypes.StringType,false),
				DataTypes.createStructField("city",DataTypes.StringType,false)
		});
		
		Dataset<Row> tempDataFields = spSession.createDataFrame(rowRDD,schema);
		
		tempDataFields.show();
		
		//working with csv data ,with sql stmt
		//provided the data is kept in table like format
		//the persistence will be only till the end of the pgm meaning temporary
		
		autoData.createOrReplaceTempView("autos");
		System.out.println("Temp Table Contents:");
		
		spSession.sql("select * from autos");
		
		
	}

}
