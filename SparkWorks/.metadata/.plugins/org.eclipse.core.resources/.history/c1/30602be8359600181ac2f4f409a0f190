package com.training.sparkworks;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import com.training.commons.DataSource;
import com.training.commons.SparkConnection;

public class SparkOperationsClient {

	public static void main(String[] args) {
		Logger.getLogger("org").setLevel(Level.ERROR);
		//optional
		Logger.getLogger("akka").setLevel(Level.ERROR);
		
		JavaSparkContext sparkContext = SparkConnection.getContext();
		
		//start loading the data
		//1.load the collection and cache it
		JavaRDD<Integer> collData = DataSource.getCollData();
		System.out.println("Total Number of Records"+collData.count());
		//2.load the file and cache it
	}

}
